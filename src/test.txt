CHAPTER 7. REGULARIZATION FOR DEEP LEARNING
of regularization. As with L2 weight decay, L1 weight decay controls the strength
of the regularization by scaling the penalty Ω using a positive hyperparameter α.
˜
Thus, the regularized objective function J(w;X,y) is given by
J˜(w;X,y) = α w +J(w;X,y), (7.19)
1
|| ||
with the corresponding gradient (actually, sub-gradient):
˜
J(w;X,y) = αsign(w)+ J(X,y;w) (7.20)
w w
∇ ∇
where sign(w) is simply the sign of w applied element-wise.
By inspecting equation 7.20, we can see immediately that the effect of L1
regularization is quite different from that of L2 regularization. Specifically, we can
see that the regularization contribution to the gradient no longer scales linearly
with each w; instead it is a constant factor with a sign equal to sign(w ). One
i i
consequence of this form of the gradient is that we will not necessarily see clean
algebraic solutions to quadratic approximations of J(X,y;w) as we did for L2
regularization.
Our simple linear model has a quadratic cost function that we can represent
via its Taylor series. Alternately, we could imagine that this is a truncated Taylor
series approximating the cost function of a more sophisticated model. The gradient
in this setting is given by
Jˆ(w) = H(w w ), (7.21)
w ∗
∇ −
where, again, H is the Hessian matrix of J with respect to w evaluated at w .
∗
Because the L1 penalty does not admit clean algebraic expressions in the case
of a fully general Hessian, we will also make the further simplifying assumption
that the Hessian is diagonal, H = diag([H ,...,H ]), where each H > 0.
1,1 n,n i,i
This assumption holds if the data for the linear regression problem has been
preprocessed to remove all correlation between the input features, which may be
accomplished using PCA.
Our quadratic approximation of the L1 regularized objective function decom-
poses into a sum over the parameters:
1
Jˆ (w;X,y) = J(w ;X,y)+ H (w w )2 +α w . (7.22)
∗
2
i,i i
−
∗i
|
i
|
i  

Theproblem ofminimizing thisapproximatecostfunction hasan analyticalsolution
(for each dimension i), with the following form:
α
w = sign(w )max w ,0 . (7.23)
i i∗ ∗i
| |− H
i,i
 
235